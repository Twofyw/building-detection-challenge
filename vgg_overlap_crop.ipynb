{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from fastai.conv_learner import *\n",
    "from fastai.dataset import *\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ORIGINAL_SIZE = 650\n",
    "\n",
    "BASE_TRAIN_DIR = \"data/train\"\n",
    "WORKING_DIR = \"data/working\"\n",
    "IMAGE_DIR = \"data/working/images/full_rgb/\"\n",
    "\n",
    "\n",
    "SOURCE_DIR = Path(\"data/working/images/full_rgb/\")\n",
    "TARGET_DIR = Path(\"data/working/images/overlap_rgb/\")\n",
    "LIST = Path('list')\n",
    "MASK = Path('mask')\n",
    "\n",
    "# Input files\n",
    "FMT_TRAIN_SUMMARY_PATH = str(\n",
    "    Path(BASE_TRAIN_DIR) /\n",
    "    Path(\"{prefix:s}_Train/\") /\n",
    "    Path(\"summaryData/{prefix:s}_Train_Building_Solutions.csv\"))\n",
    "FMT_TRAIN_RGB_IMAGE_PATH = str(\n",
    "    Path(\"{datapath:s}/\") /\n",
    "    Path(\"RGB-PanSharpen/RGB-PanSharpen_{image_id:s}.tif\"))\n",
    "FMT_TEST_RGB_IMAGE_PATH = str(\n",
    "    Path(\"{datapath:s}/\") /\n",
    "    Path(\"RGB-PanSharpen/RGB-PanSharpen_{image_id:s}.tif\"))\n",
    "FMT_TRAIN_MSPEC_IMAGE_PATH = str(\n",
    "    Path(\"{datapath:s}/\") /\n",
    "    Path(\"MUL-PanSharpen/MUL-PanSharpen_{image_id:s}.tif\"))\n",
    "FMT_TEST_MSPEC_IMAGE_PATH = str(\n",
    "    Path(\"{datapath:s}/\") /\n",
    "    Path(\"MUL-PanSharpen/MUL-PanSharpen_{image_id:s}.tif\"))\n",
    "\n",
    "# Preprocessing result\n",
    "FMT_RGB_BANDCUT_TH_PATH = IMAGE_DIR + \"/rgb_bandcut{}.csv\"\n",
    "FMT_MUL_BANDCUT_TH_PATH = IMAGE_DIR + \"/mul_bandcut{}.csv\"\n",
    "\n",
    "# Image list, Image container and mask container\n",
    "FMT_VALTRAIN_IMAGELIST_PATH = IMAGE_DIR + \"/{prefix:s}_valtrain_ImageId.csv\"\n",
    "FMT_VALTRAIN_MASK_STORE = IMAGE_DIR + \"/valtrain_{}_mask.h5\"\n",
    "FMT_VALTRAIN_IM_STORE = IMAGE_DIR + \"/valtrain_{}_im.h5\"\n",
    "FMT_VALTRAIN_MUL_STORE = IMAGE_DIR + \"/valtrain_{}_mul.h5\"\n",
    "\n",
    "FMT_VALTEST_IMAGELIST_PATH = IMAGE_DIR + \"/{prefix:s}_valtest_ImageId.csv\"\n",
    "FMT_VALTEST_MASK_STORE = IMAGE_DIR + \"/valtest_{}_mask.h5\"\n",
    "FMT_VALTEST_IM_STORE = IMAGE_DIR + \"/valtest_{}_im.h5\"\n",
    "FMT_VALTEST_MUL_STORE = IMAGE_DIR + \"/valtest_{}_mul.h5\"\n",
    "\n",
    "FMT_IMMEAN = IMAGE_DIR + \"/{}_immean.h5\"\n",
    "FMT_MULMEAN = IMAGE_DIR + \"/{}_mulmean.h5\"\n",
    "\n",
    "FMT_TEST_IMAGELIST_PATH = IMAGE_DIR + \"/{prefix:s}_test_ImageId.csv\"\n",
    "FMT_TEST_IM_STORE = IMAGE_DIR + \"/test_{}_im.h5\"\n",
    "FMT_TEST_MUL_STORE = IMAGE_DIR + \"/test_{}_mul.h5\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sz = 192\n",
    "# batch size = 30: GPU memory usage: 9751MiB / 11441MiB\n",
    "\n",
    "bs = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-cut images to 192x192 with stride 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not TARGET_DIR.exists():\n",
    "    (TARGET_DIR / Path('list')).mkdir(parents=True)\n",
    "    (TARGET_DIR / Path('mask')).mkdir(parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use parallel if too slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stride = 48\n",
    "lists = sorted((SOURCE_DIR / Path('list')).glob('*'))\n",
    "masks = sorted((SOURCE_DIR / Path('mask')).glob('*'))\n",
    "for i, img in enumerate(lists):\n",
    "    lst = plt.imread(str(img))\n",
    "    msk = plt.imread(str(masks[i]))\n",
    "\n",
    "    for x in np.arange(ORIGINAL_SIZE, step=stride):\n",
    "        for y in np.arange(ORIGINAL_SIZE, step=stride):\n",
    "            if (y + sz) > ORIGINAL_SIZE:\n",
    "                y = ORIGINAL_SIZE - sz - 1\n",
    "            if (x + sz) > ORIGINAL_SIZE:\n",
    "                x = ORIGINAL_SIZE - sz - 1\n",
    "                    \n",
    "            lst_patch = np.copy(lst[x:x + sz, y:y + sz])\n",
    "            msk_left = sz/2 - stride/2\n",
    "            msk_patch = np.copy(msk[int(x + msk_left):int(x + msk_left + stride),\n",
    "                                   int(y + msk_left):int(y + msk_left + stride)])\n",
    "            \n",
    "            plt.imsave(TARGET_DIR / Path('list') / Path(img.name[:-4] + '_' + str(x) + '_' + str(y) + '.png'), lst_patch)\n",
    "            plt.imsave(TARGET_DIR / Path('mask') / Path(img.name[:-4] + '_' + str(x) + '_' + str(y) + '.png'), msk_patch)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save to ramdisk if io bottlenecks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatchedFilesDataset(FilesDataset):\n",
    "    def __init__(self, fnames, y, transform, path):\n",
    "        self.y=y\n",
    "        assert(len(fnames)==len(y))\n",
    "        super().__init__(fnames, transform, path)\n",
    "    def get_y(self, i): return open_image(os.path.join(self.path, self.y[i]))\n",
    "    def get_c(self): return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_DIR.glob()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_names = np.array((TARGET_DIR / LIST).glob()); x_names.sort()\n",
    "y_names = np.array((TARGET_DIR / MASK).glob()); y_names.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/data/working/images/v5/256/AOI_3_Paris_img10.jpg',\n",
       " '/data/working/images/v5/256/AOI_3_Paris_img10.png')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(zip(x_names, y_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80:20 split\n",
    "val_idxs = np.random.permutation(range(230)) \n",
    "((val_x,trn_x),(val_y,trn_y)) = split_by_idx(val_idxs, x_names, y_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
